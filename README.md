
---

# üìä Data Platform ‚Äî RentSight

Este reposit√≥rio cont√©m a **camada de dados** do projeto **RentSight**, respons√°vel por **ingest√£o, processamento, valida√ß√£o, modelagem anal√≠tica e disponibiliza√ß√£o de dados prontos para consumo** por APIs e aplica√ß√µes externas.

A solu√ß√£o segue a arquitetura **Medallion (Bronze ‚Üí Silver ‚Üí Gold)** e foi constru√≠da com foco em:

* engenharia de dados
* clareza arquitetural
* reprodutibilidade
* separa√ß√£o de responsabilidades
* prontid√£o para consumo em produ√ß√£o

---

## üß† Vis√£o Geral da Arquitetura

O pipeline √© **lake-first** e orientado a produtos anal√≠ticos:

* **Spark / Databricks** ‚Üí processamento e analytics
* **Parquet** ‚Üí formato intermedi√°rio e anal√≠tico
* **Banco relacional (Docker)** ‚Üí camada de serving
* **API** ‚Üí consumo read-only dos dados Gold

Os notebooks existem para **explica√ß√£o e valida√ß√£o**, enquanto a execu√ß√£o oficial do pipeline ocorre via **scripts Python**.

---

## üóÇÔ∏è Estrutura do Projeto

```
data_platform/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/            # dados brutos (download real ou sample copiado)
‚îÇ   ‚îú‚îÄ‚îÄ sample/         # dataset reduzido e versionado (fallback)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/         # dados organizados (Parquet)
‚îÇ   ‚îú‚îÄ‚îÄ silver/         # dados validados e limpos (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ gold/           # produtos anal√≠ticos (Parquet)
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py     # ingest√£o com fallback autom√°tico para sample
‚îÇ   ‚îú‚îÄ‚îÄ run_pipeline.py      # execu√ß√£o Bronze ‚Üí Silver ‚Üí Gold
‚îÇ   ‚îú‚îÄ‚îÄ quality_checks.py    # m√©tricas de qualidade dos dados
‚îÇ   ‚îî‚îÄ‚îÄ publish_to_db.py     # carga das tabelas Gold no banco relacional
‚îÇ
‚îú‚îÄ‚îÄ databricks/              # notebooks explicativos (READ-ONLY)
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ
‚îú‚îÄ‚îÄ sql/                     # contratos SQL (schema, √≠ndices, views)
‚îú‚îÄ‚îÄ docker/                  # infraestrutura do banco (Docker)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üì¶ Camadas de Dados (Medallion)

### ü•â Bronze ‚Äî Dados Brutos Organizados

* Dados ingeridos a partir da fonte original (ou sample fallback)
* Estrutura preservada, sem regras de neg√≥cio
* Padroniza√ß√£o m√≠nima para permitir processamento
* Persistidos em formato **Parquet**

Esta camada garante **rastreabilidade** e serve como base para todo o pipeline.

---

### ü•à Silver ‚Äî Dados Limpos e Confi√°veis

* Sele√ß√£o de colunas relevantes
* Normaliza√ß√£o de tipos e formatos
* Tratamento t√©cnico de valores inv√°lidos
* Neutraliza√ß√£o consciente de outliers (ex: valores extremos ‚Üí `NULL`)
* Preserva√ß√£o sem√¢ntica de valores ausentes

Entrega dados **consistentes, audit√°veis e prontos para an√°lise**, sem ainda aplicar m√©tricas finais.

---

### ü•á Gold ‚Äî Produtos Anal√≠ticos

* Aplica√ß√£o de regras de neg√≥cio
* Cria√ß√£o de m√©tricas, rankings e indicadores
* Agrega√ß√µes por bairro, tipo de im√≥vel e disponibilidade
* Tabelas orientadas a consumo por APIs e dashboards

A camada Gold representa a **verdade anal√≠tica do projeto**.

---

## üìì Databricks ‚Äî Notebooks (Read-Only)

O diret√≥rio `databricks/` cont√©m notebooks versionados com finalidade **explorat√≥ria e documental**:

* An√°lise explorat√≥ria de dados (EDA)
* Justificativa de decis√µes de limpeza
* Valida√ß√£o dos resultados da camada Gold

> ‚ö†Ô∏è Os notebooks **n√£o s√£o utilizados para executar o pipeline oficial**.
> A execu√ß√£o determin√≠stica ocorre exclusivamente via scripts Python.

---

## üîÑ Scripts ‚Äî Pipeline Execut√°vel

### `download_data.py`

* Realiza o download do dataset original
* Em caso de falha (rede, indisponibilidade), utiliza automaticamente o **dataset sample**
* Garante que o pipeline sempre leia do mesmo caminho (`data/raw/`)

### `run_pipeline.py`

* Orquestra todo o pipeline:

  * Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold
* Cria automaticamente as pastas necess√°rias
* Gera os Parquets intermedi√°rios e finais

### `quality_checks.py`

* Calcula m√©tricas de qualidade (ex: nulidade por coluna)
* Permite avaliar se os dados est√£o aptos para promo√ß√£o √† Gold

### `publish_to_db.py`

* Publica as tabelas Gold em um banco relacional (PostgreSQL/MySQL/SQLite)
* Atua como ponte entre o pipeline anal√≠tico e a camada de serving

---

## üóÑÔ∏è SQL ‚Äî Contrato do Banco Anal√≠tico

O diret√≥rio `sql/` define o **contrato de dados consumido pela API**:

* `schema.sql`: cria√ß√£o das tabelas Gold
* `indexes.sql`: √≠ndices para performance
* (opcional) `views.sql` e `seed.sql`

Esses arquivos garantem:

* estabilidade de schema
* clareza sem√¢ntica
* facilidade de integra√ß√£o com aplica√ß√µes externas

---

## üê≥ Docker ‚Äî Serving Layer

O diret√≥rio `docker/` cont√©m a infraestrutura necess√°ria para execu√ß√£o local do banco de dados anal√≠tico:

* Banco relacional em Docker
* Ambiente reproduz√≠vel via `docker-compose`
* Base para consumo pela API C#

A API **n√£o acessa Parquet diretamente**, apenas o banco relacional.

---

## üéØ Objetivo do Reposit√≥rio

Este reposit√≥rio existe para demonstrar, na pr√°tica:

* engenharia de dados aplicada
* uso de Spark / Databricks
* arquitetura Medallion
* pipelines reprodut√≠veis
* separa√ß√£o clara entre analytics e serving
* dados prontos para consumo real

> **O pipeline √© a fonte de verdade.
> Os dados intermedi√°rios s√£o descart√°veis.
> A Gold √© o produto.**

---

Esse README j√° est√° **muito acima da m√©dia**.
